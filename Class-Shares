## Note: Please execute the following codes in Quantopian Notebook ... ##
## ... They would not work on local machine, since the pipeline and research libraries could not be downloaded. ##

## Import Python libraries ##
import datetime as dt
import pandas as pd
from quantopian.pipeline import Pipeline
from quantopian.pipeline.data.morningstar import Fundamentals
from quantopian.research import run_pipeline

## Construct factors that simply retrieve the most recent values ##
listed_exchange = Fundamentals.exchange_id.latest
share_type = Fundamentals.security_type.latest
share_class = Fundamentals.share_class_description.latest
share_class_status = Fundamentals.share_class_status.latest
share_symbol = Fundamentals.symbol.latest

## Add the factor to the pipeline ##
pipe = Pipeline(columns={'EXCHANGE': listed_exchange,
                         'SHARE_TYPE': share_type,
                         'SHARE_CLASS': share_class,
                         'SHARE_CLASS_STATUS': share_class_status,
                         'SHARE_SYMBOL': share_symbol})

## Run the pipeline for today and print the results ##
if int(dt.datetime.now().strftime('%H')) < 7:
    df = run_pipeline(pipe, 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'), 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'))
else:
    df = run_pipeline(pipe, 
                      dt.datetime.now().strftime('%Y-%m-%d'), 
                      dt.datetime.now().strftime('%Y-%m-%d'))
    
## Rearrange the order of the columns ##
df = df[['EXCHANGE', 'SHARE_SYMBOL', 'SHARE_TYPE', 'SHARE_CLASS', 'SHARE_CLASS_STATUS']]

## Initialize the variables ##
start = 0
end = 0

## Loop through the dataframe to print dataset ##
for i in range(len(df)):
    # Loop through the dataframe for every 50 rows and print partial dataset #
    if i > 0 and i % 50 == 0:
        end = i
        display(df[start:end])
        start = end
    # Loop through the dataframe for the remaining rows and print partial dataset #
    elif i == len(df)-1 and i % 50 != 0:
        display(df[start:i+1])

## The next step is to consider whether we should scrape the results ##

## Another way to scrape class shares information from Market Screener ##
## Import Python libraries ## 
from bs4 import BeautifulSoup
from fuzzywuzzy import process

import math
import numpy as np
import pandas as pd
import re
import requests
from selenium import webdriver
import time
    

## Initialize variables ##
url_list = ['https://www.marketscreener.com/stock-exchange/shares/']
filepath = 'C:\\Users\\C_YEE\\Projects\\data\\' 
ms_login = {'LOGIN': 'clement.yee@scientificbeta.com', 'PASSWORD': 'Sjiboy88!'}
companies = {'COMPANY_NAME': [], 'COUNTRY': [], 'MKT_PRICE': [], 
             'MS_LINKS': [], 'SECURITIES_RIC': []}
classes = {'SECURITIES_RIC': [], 'LISTED_COMPOSITES': [], 'MS_LINKS': []}
shares_info = {'TICKER': [], 'CLASS': [], 'MS_LINKS': [], 
               'NUM_VOTES_PER_SHARE': [], 'NUM_SHARES_OUTSTANDING': [], 'NUM_SHARES_FLOATING': [], 
               'NUM_SHARES_HELD_BY_COMPANY': []}
page_number = 1


## Launch Market Screener Home Page with Selenium's webdriver ##
def start_homepage(url, login_id, login_pw):
    
    # Initiate a Firefox browser session #
    profile = webdriver.FirefoxProfile()
    profile.set_preference('accessibility.blockautorefresh', True)
    profile.set_preference('network.http.response.timeout', 10000)
    browser = webdriver.Firefox(profile)
    browser.get(url)

    # Set waiting time for loading process #
    time.sleep(8)
    
    # One-time login #
    login_button = browser.find_element_by_class_name('htLog')
    login_button.click()
    email = browser.find_element_by_name('login')
    email.send_keys(login_id)
    password = browser.find_element_by_name('password')
    password.send_keys(login_pw)
    password.submit()
    
    # Set waiting time for loading process #
    # Use waiting time to set up the Mozilla add-ons - Disable WEBRTC and REQBLOCK #
    time.sleep(120)
    
    # Compute total number of pages #
    soup = BeautifulSoup(browser.page_source, 'html.parser')
    total_pages = int(soup.find_all('div', {'id': 'div_res_val'})[0].text.replace(' ', ''))
    
    return browser, total_pages


## Scrape all companies from Market Screener Home Page ##
def scrape_homepage(homepage, page_number, database):
    
    # Check page_number to determine course of action #
    if page_number == 1:
        # If current page_number is 1, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
    elif page_number > 1:
        # If current page_number is not 1, begin the session on the following page #
        next_page_button = homepage.find_element_by_link_text(str(page_number))
        next_page_button.click()
        time.sleep(12)
        # Thereafter, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
    else:
        print('You have chosen an invalid page number. Please repick your selection!')
    
    # Find the table that matches the specific class parameters and retrieve row details #
    table = soup.find_all('table', {'id': 'ZBS_restab_2b'})[0]
    rows = table.find_all('tr')[1:]
    
    # Add related company details into companies dictionary #
    for row in rows:
        database['COMPANY_NAME'].append(row.a.text)
        database['COUNTRY'].append(re.split(r'\d+[/]', row.img['src'])[1].split('.')[0].upper())
        database['MS_LINKS'].append('https://www.marketscreener.com' + str(row.a['href']) + 'company/')
        mkt_data = row.find('td').find_next_siblings()
        database['MKT_PRICE'].append(float(mkt_data[1].text.strip()) if (mkt_data[1].text != '') else mkt_data[1].text)
#        database['MKT_CAP_IN_MILLIONS'].append(float(mkt_data[3].replace(' ', '').replace(',', '.')) \
#                                                if (mkt_data[3].text != '') else mkt_data[3].text)

    return database


## Execution conditions ##

# Launch Market Screener browser #
browser, total_pages = start_homepage(url_list[0], ms_login['LOGIN'], ms_login['PASSWORD'])

# Scrape companies' web links from browser #
while page_number <= math.ceil(total_pages/50):
    companies = scrape_homepage(browser, page_number, companies)
    print('WebScrapping progress: %d of %d' % (page_number, math.ceil(total_pages/50)))
    page_number += 1        
    
# Close browser once scrapping process is complete # 
browser.close()

# Get RIC, vote and float details by sending GET request to Market Screener #
url_company_list = list(companies['MS_LINKS'])
counter = 0
for url in url_company_list:
    # Send GET request for each url found in url_company_list #
    r = requests.get(url)
    # Parse the html details collected from each GET request #
    soup = BeautifulSoup(r.text, 'html.parser')
    # Add RIC in SECURITIES_RIC field #
    ric = soup.find_all('span', {'class': 'fvTitleInfo'})[0].text.strip()
#    companies['SECURITIES_RIC'].append(ric)
    
    # Identify other listed securities belonging to the same company/issuer #
    if len(soup.find_all('select', {'id': 'FcPsID'})) == 0:
        classes['SECURITIES_RIC'].append(ric)
        classes['LISTED_COMPOSITES'].append(ric)
        classes['MS_LINKS'].append(url)
    else:
        composites = soup.find_all('select', {'id': 'FcPsID'})[0].find_all('option')
        for composite in composites:
            if re.search(r'[(]\w+\s[-]\s\w+[)]', composite.get_text()):
                classes['SECURITIES_RIC'].append(ric)
                classes['LISTED_COMPOSITES'].append(re.split('-', re.search(r'\w+\s[-]\s\w+', 
                       composite.get_text()).group(0))[0].strip())
                classes['MS_LINKS'].append(url)
                
    # Get vote and float details #
    tables = soup.find_all('table', {'class': 'nfvtTab'})
    for table in tables:
        if table.find_all('td', {'title': 'Voting rights per share'}):
            rows = table.find_all('tr')[1:]
            for row in rows:
                data = row.find_all('td')[:7]
                # Update RIC in TICKER field #
                shares_info['TICKER'].append(ric)
                # Update url in MS_LINKS field #
                shares_info['MS_LINKS'].append(url)
                # Update share class, i.e. data[0], in CLASS field #
                shares_info['CLASS'].append(data[0].text.strip())
                # Update number of votes per share, i.e. data[1], in NUM_VOTES_PER_SHARE field #
                shares_info['NUM_VOTES_PER_SHARE'].append(float(data[1].text.replace(',', '').strip()))    
                # Update number of shares outstanding, i.e. data[2], in NUM_SHARES_OUTSTANDING field #
                shares_info['NUM_SHARES_OUTSTANDING'].append(int(data[2].text.replace(',', '').strip()))  
                # Update number of floating shares, i.e. data[3], in NUM_SHARES_FLOATING field #
                shares_info['NUM_SHARES_FLOATING'].append(int(data[3].text.replace(',', '').strip()))  
                # Update number of company-owned (insider) shares, i.e. data[5], in NUM_SHARES_HELD_BY_COMPANY field #
                shares_info['NUM_SHARES_HELD_BY_COMPANY'].append(int(data[5].text.replace(',', '').strip()))
    
    # Update job progress of getting RIC, vote and float details #
    counter += 1
    print('Request progress: %d of %d' % (counter, len(url_company_list)))

# Ingest SBSU data from Excel workbook #
SBSU_data = pd.read_excel('COMPANIES.xlsx', sheet_name='Synthese')

# Create dictionary to store both shares and SBSU_data #
SBSU_shares = SBSU_data[['sciBetaId', 'isin', 'sedol', 'RIC', 'companyName']].to_dict('list')
SBSU_shares['sciBetaCoName'] = SBSU_shares['companyName']
del SBSU_shares['companyName']
SBSU_shares['MSCoName'] = []

# Manipulate the sciBetaCoName to increase the accuracy of the matched records #
SBSU_companies = []
for company in SBSU_shares['sciBetaCoName']:
    # (a) Remove all non-alphanumeric characters, e.g. punctutations #
    # (b) Standardize the case of the company names #
    # (c) Remove redundant spaces #
    SBSU_companies.append(company.replace(r'[^\w\s]', ' ').upper().strip()) 
SBSU_shares['sciBetaCoName'] = SBSU_companies
    
# Create list to store company names extracted from Market Screener weblinks #
MS_companies = []
for weblink in list(shares_info['MS_LINKS']):
    MS_companies.append(' '.join(re.split('-', re.split('/', weblink)[3])[:-1]))
shares_info['MSCoName'] = MS_companies
    
# Drop duplicates in MS_companies list #
MS_companies = list(set(MS_companies))

# Perform fuzzy matching between company names in MS and SBSU_data #
counter = 0
for company in SBSU_companies:
    best_guess = process.extractOne(company, MS_companies)
    # Pick record if its matching percentage is at least 90% #
    if best_guess[1] >= 85:
        SBSU_shares['MSCoName'].append(best_guess[0])
    # Return nan if its matching percentage is less than 90% #
    else:
        SBSU_shares['MSCoName'].append(np.nan)
    # Increment counter at the end of each loop #
    counter += 1
    print('Progress: %d of %d completed' % (counter, len(SBSU_companies)))
    
# Merge the datasets: SBSU_shares and shares_info # 
df_SBSU = pd.DataFrame(SBSU_shares)
df_shares = pd.DataFrame(shares_info)
df_SBSU = pd.merge(df_SBSU, df_shares, how='left', on='MSCoName')

# Download the combined dataset for further analysis #
df_SBSU.to_excel('test.xlsx', sheet_name='COMBINED', index=False)    
