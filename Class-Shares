## Note: Please execute the following codes in Quantopian Notebook ... ##
## ... They would not work on local machine, since the pipeline and research libraries could not be downloaded. ##

## Import Python libraries ##
import datetime as dt
import pandas as pd
from quantopian.pipeline import Pipeline
from quantopian.pipeline.data.morningstar import Fundamentals
from quantopian.research import run_pipeline

## Construct factors that simply retrieve the most recent values ##
listed_exchange = Fundamentals.exchange_id.latest
share_type = Fundamentals.security_type.latest
share_class = Fundamentals.share_class_description.latest
share_class_status = Fundamentals.share_class_status.latest
share_symbol = Fundamentals.symbol.latest

## Add the factor to the pipeline ##
pipe = Pipeline(columns={'EXCHANGE': listed_exchange,
                         'SHARE_TYPE': share_type,
                         'SHARE_CLASS': share_class,
                         'SHARE_CLASS_STATUS': share_class_status,
                         'SHARE_SYMBOL': share_symbol})

## Run the pipeline for today and print the results ##
if int(dt.datetime.now().strftime('%H')) < 7:
    df = run_pipeline(pipe, 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'), 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'))
else:
    df = run_pipeline(pipe, 
                      dt.datetime.now().strftime('%Y-%m-%d'), 
                      dt.datetime.now().strftime('%Y-%m-%d'))
    
## Rearrange the order of the columns ##
df = df[['EXCHANGE', 'SHARE_SYMBOL', 'SHARE_TYPE', 'SHARE_CLASS', 'SHARE_CLASS_STATUS']]

## Initialize the variables ##
start = 0
end = 0

## Loop through the dataframe to print dataset ##
for i in range(len(df)):
    # Loop through the dataframe for every 50 rows and print partial dataset #
    if i > 0 and i % 50 == 0:
        end = i
        display(df[start:end])
        start = end
    # Loop through the dataframe for the remaining rows and print partial dataset #
    elif i == len(df)-1 and i % 50 != 0:
        display(df[start:i+1])

## The next step is to consider whether we should scrape the results ##

## Another way to scrape class shares information from Market Screener ##
## Import Python libraries ## 
from bs4 import BeautifulSoup
import datetime as dt
import Levenshtein as lev
import numpy as np
import os
import pandas as pd
import re
import requests
import xml.etree.ElementTree as ET
    
## Initialize variables ##
ms_url = 'https://www.marketscreener.com/search/instruments/equities/?aComposeInputSearch=s_'
filepath = 'C:\\Users\\C_YEE\\Projects\\data\\News' 

## Load Scientific Beta Stock Universe ##
SBSU_data = pd.read_excel(filepath + '\\SBSU.xlsx')
SBSU_ticker_list = list(SBSU_data['BBG_STOCK_TICKER']) 

## GET request ##
for ticker in SBSU_ticker_list[1332:1333]:
    # Assign ticker's country to 'country' variable #
    country = SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER']==ticker,['country']]['country'].iloc[0]
    # Send GET request for each ticker #
    r = requests.get(ms_url + str(ticker))
    # Parse the html details collected from GET request #
    first_soup = BeautifulSoup(r.text, 'html.parser')
    # Find the first table that matches the specific class parameters #
    table = first_soup.find_all('table', {'class': 'tabBodyLV17'})[0]
    # Get all row details from the first table #
    rows = table.find_all('tr')[1:]
    for row in rows:
        data = row.find('td').find_next_siblings()
        if country in data[0].img['title']:
            url = 'https://www.marketscreener.com' + str(data[1].a['href']) + 'company/'
            r = requests.get(url)
            second_soup = BeautifulSoup(r.text, 'html.parser')
            tables = second_soup.find_all('table', {'class': 'nfvtTab'})
            for table in tables:
                if table.find_all('td', {'title': 'Voting rights per share'}):
                    rows = table.find_all('tr')[1:]


## Import Python libraries ## 
from bs4 import BeautifulSoup
import datetime as dt
import Levenshtein as lev
import math
import numpy as np
import pandas as pd
import re
import requests
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
    

## Initialize variables ##
url_list = ['https://www.marketscreener.com/stock-exchange/shares/',
            'https://www.marketscreener.com/search/instruments/equities/?aComposeInputSearch=s_']
filepath = 'C:\\Users\\C_YEE\\Projects\\data' 
ms_login = {'LOGIN': 'clement.yee@scientificbeta.com', 'PASSWORD': 'Sjiboy88!'}
companies = {'COMPANY_NAME': [], 'COUNTRY': [], 'MKT_PRICE': [], 
             'MKT_CAP_IN_MILLIONS': [], 'MS_LINKS': [], 'SECURITIES_RIC': []}
page_number = 1

## Launch Market Screener Home Page with Selenium's webdriver ##
def start_homepage(url, login_id, login_pw):
    
    # Initiate a Firefox browser session #
    profile = webdriver.FirefoxProfile()
    profile.set_preference('accessibility.blockautorefresh', True)
    profile.set_preference('network.http.response.timeout', 10000)
    browser = webdriver.Firefox(profile)
    browser.get(url)

    # Set waiting time for loading process #
    time.sleep(8)
    
    # One-time login #
    login_button = browser.find_element_by_class_name('htLog')
    login_button.click()
    email = browser.find_element_by_name('login')
    email.send_keys(login_id)
    password = browser.find_element_by_name('password')
    password.send_keys(login_pw)
    password.submit()
    
    # Set waiting time for loading process #
    time.sleep(8)
    
    soup = BeautifulSoup(browser.page_source, 'html.parser')
    total_pages = int(soup.find_all('div', {'id': 'div_res_val'})[0].text.replace(' ', ''))
    return browser, total_pages


## Disable Javascript and put the program to temporary rest ##
#def js_sleep():
    

## Scrape all companies from Market Screener Home Page ##
def scrape_homepage(homepage, page_number, database = companies):
    
    # Check page_number to determine course of action #
    if page_number == 1:
        # If current page_number is 1, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
        #  do nothing #
    elif page_number > 1:
        # If current page_number is not 1, begin the session on the following page #
        #next_page_button = WebDriverWait(homepage,8).until(EC.element_to_be_clickable((By.LINK_TEXT, str(page_number))))
        next_page_button = homepage.find_element_by_link_text(str(page_number))
        next_page_button.click()
        time.sleep(13)
        
        # Thereafter, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
    else:
        print('You have chosen an invalid page number. Please repick your selection!')
    
    # Find the table that matches the specific class parameters and retrieve row details #
    table = soup.find_all('table', {'id': 'ZBS_restab_2b'})[0]
    rows = table.find_all('tr')[1:]
    
    # Add related company details into companies dictionary #
    for row in rows:
        database['COMPANY_NAME'].append(row.a.text)
        database['COUNTRY'].append(re.split(r'\d+[/]', row.img['src'])[1].split('.')[0].upper())
        database['MS_LINKS'].append('https://www.marketscreener.com' + str(row.a['href']) + 'company/')
        mkt_data = row.find('td').find_next_siblings()
        database['MKT_PRICE'].append(float(mkt_data[1].text.strip()) if (mkt_data[1].text != '') else mkt_data[1].text)
        database['MKT_CAP_IN_MILLIONS'].append(float(mkt_data[3].text.replace(' ', '')) \
                                                if (mkt_data[3].text != '') else mkt_data[3].text)

    return database

## Execution conditions ##
browser, total_pages = start_homepage(url_list[0], ms_login['LOGIN'], ms_login['PASSWORD'])
while page_number <= math.ceil(total_pages/50):
    companies = scrape_homepage(browser, page_number)
    print('WebScrapping progress: %d of %d' % (page_number, math.ceil(total_pages/50)))
    page_number += 1        
browser.close()
