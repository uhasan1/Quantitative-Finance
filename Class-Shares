## Note: Please execute the following codes in Quantopian Notebook ... ##
## ... They would not work on local machine, since the pipeline and research libraries could not be downloaded. ##

## Import Python libraries ##
import datetime as dt
import pandas as pd
from quantopian.pipeline import Pipeline
from quantopian.pipeline.data.morningstar import Fundamentals
from quantopian.research import run_pipeline

## Construct factors that simply retrieve the most recent values ##
listed_exchange = Fundamentals.exchange_id.latest
share_type = Fundamentals.security_type.latest
share_class = Fundamentals.share_class_description.latest
share_class_status = Fundamentals.share_class_status.latest
share_symbol = Fundamentals.symbol.latest

## Add the factor to the pipeline ##
pipe = Pipeline(columns={'EXCHANGE': listed_exchange,
                         'SHARE_TYPE': share_type,
                         'SHARE_CLASS': share_class,
                         'SHARE_CLASS_STATUS': share_class_status,
                         'SHARE_SYMBOL': share_symbol})

## Run the pipeline for today and print the results ##
if int(dt.datetime.now().strftime('%H')) < 7:
    df = run_pipeline(pipe, 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'), 
                      (dt.datetime.now() + dt.timedelta(-1)).strftime('%Y-%m-%d'))
else:
    df = run_pipeline(pipe, 
                      dt.datetime.now().strftime('%Y-%m-%d'), 
                      dt.datetime.now().strftime('%Y-%m-%d'))
    
## Rearrange the order of the columns ##
df = df[['EXCHANGE', 'SHARE_SYMBOL', 'SHARE_TYPE', 'SHARE_CLASS', 'SHARE_CLASS_STATUS']]

## Initialize the variables ##
start = 0
end = 0

## Loop through the dataframe to print dataset ##
for i in range(len(df)):
    # Loop through the dataframe for every 50 rows and print partial dataset #
    if i > 0 and i % 50 == 0:
        end = i
        display(df[start:end])
        start = end
    # Loop through the dataframe for the remaining rows and print partial dataset #
    elif i == len(df)-1 and i % 50 != 0:
        display(df[start:i+1])

## The next step is to consider whether we should scrape the results ##

## Another way to scrape class shares information from Market Screener ##
## Import Python libraries ## 
from bs4 import BeautifulSoup
import datetime as dt
import Levenshtein as lev
import math
import numpy as np
import pandas as pd
import re
import requests
from selenium import webdriver
import time
    

## Initialize variables ##
url_list = ['https://www.marketscreener.com/stock-exchange/shares/',
            'https://www.marketscreener.com/search/instruments/equities/?aComposeInputSearch=s_']
filepath = 'C:\\Users\\C_YEE\\Projects\\data\\' 
ms_login = {'LOGIN': 'clement.yee@scientificbeta.com', 'PASSWORD': 'Sjiboy88!'}
#companies = {'COMPANY_NAME': [], 'COUNTRY': [], 'MKT_PRICE': [], 
#             'MS_LINKS': [], 'SECURITIES_RIC': []}
classes = {'SECURITIES_RIC': [], 'LISTED_COMPOSITES': []}
shares_info = {'TICKER': [], 'CLASS': [], 'VOTES_PER_SHARE': [], 
               'SHARES_OUTSTANDING': [], 'SHARES_FLOATING': [], 'SHARES_HELD_BY_COMPANY': []}
page_number = 1


## Launch Market Screener Home Page with Selenium's webdriver ##
def start_homepage(url, login_id, login_pw):
    
    # Initiate a Firefox browser session #
    profile = webdriver.FirefoxProfile()
    profile.set_preference('accessibility.blockautorefresh', True)
    profile.set_preference('network.http.response.timeout', 10000)
    browser = webdriver.Firefox(profile)
    browser.get(url)

    # Set waiting time for loading process #
    time.sleep(8)
    
    # One-time login #
    login_button = browser.find_element_by_class_name('htLog')
    login_button.click()
    email = browser.find_element_by_name('login')
    email.send_keys(login_id)
    password = browser.find_element_by_name('password')
    password.send_keys(login_pw)
    password.submit()
    
    # Set waiting time for loading process #
    time.sleep(8)
    
    soup = BeautifulSoup(browser.page_source, 'html.parser')
    total_pages = int(soup.find_all('div', {'id': 'div_res_val'})[0].text.replace(' ', ''))
    return browser, total_pages


## Scrape all companies from Market Screener Home Page ##
def scrape_homepage(homepage, page_number, database = companies):
    
    # Check page_number to determine course of action #
    if page_number == 1:
        # If current page_number is 1, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
        #  do nothing #
    elif page_number > 1:
        # If current page_number is not 1, begin the session on the following page #
        next_page_button = homepage.find_element_by_link_text(str(page_number))
        next_page_button.click()
        time.sleep(12)
        # Thereafter, proceed to scrape the list of companies #
        soup = BeautifulSoup(homepage.page_source, 'html.parser')
    else:
        print('You have chosen an invalid page number. Please repick your selection!')
    
    # Find the table that matches the specific class parameters and retrieve row details #
    table = soup.find_all('table', {'id': 'ZBS_restab_2b'})[0]
    rows = table.find_all('tr')[1:]
    
    # Add related company details into companies dictionary #
    for row in rows:
        database['COMPANY_NAME'].append(row.a.text)
        database['COUNTRY'].append(re.split(r'\d+[/]', row.img['src'])[1].split('.')[0].upper())
        database['MS_LINKS'].append('https://www.marketscreener.com' + str(row.a['href']) + 'company/')
        mkt_data = row.find('td').find_next_siblings()
        database['MKT_PRICE'].append(float(mkt_data[1].text.strip()) if (mkt_data[1].text != '') else mkt_data[1].text)
#        database['MKT_CAP_IN_MILLIONS'].append(float(mkt_data[3].replace(' ', '').replace(',', '.')) \
#                                                if (mkt_data[3].text != '') else mkt_data[3].text)

    return database


## Execution conditions ##

## Launch Market Screener browser #
#browser, total_pages = start_homepage(url_list[0], ms_login['LOGIN'], ms_login['PASSWORD'])

## Scrape companies' web links from browser #
## Ensure Mozilla add-ons - 'Disable WEBRTC' and 'Reqblock' are installed once browser is initiated #
#while page_number <= math.ceil(total_pages/50):
#    companies = scrape_homepage(browser, page_number)
#    print('WebScrapping progress: %d of %d' % (page_number, math.ceil(total_pages/50)))
#    page_number += 1        
#    
## Close browser once scrapping process is complete # 
#browser.close()

# Get RIC, vote and float details by sending GET request to Market Screener #
url_company_list = list(companies['MS_LINKS'])
for url in url_company_list[:1]:
    # Send GET request for each url found in url_company_list #
    r = requests.get(url)
    # Parse the html details collected from each GET request #
    soup = BeautifulSoup(r.text, 'html.parser')
    # Add RIC in SECURITIES_RIC field #
    ric = soup.find_all('span', {'class': 'fvTitleInfo'})[0].text.strip()
    companies['SECURITIES_RIC'].append(ric)
    
    # Identify other listed securities #
    if len(soup.find_all('select', {'id': 'FcPsID'})) == 0:
        classes['SECURITIES_RIC'].append(ric)
        classes['LISTED_COMPOSITES'].append(ric)
    else:
        composites = soup.find_all('select', {'id': 'FcPsID'})[0].find_all('option')
        for composite in composites:
            if re.search(r'[(]\w+\s[-]\s\w+[)]', composite.get_text()):
                classes['SECURITIES_RIC'].append(ric)
                classes['LISTED_COMPOSITES'].append(re.split('-', re.search(r'\w+\s[-]\s\w+', 
                       composite.get_text()).group(0))[0].strip())
    
    
    tables = soup.find_all('table', {'class': 'nfvtTab'})
    for table in tables:
        if table.find_all('td', {'title': 'Voting rights per share'}):
            rows = table.find_all('tr')[1:]
                    
    
### Load Scientific Beta Stock Universe ##
#SBSU_data = pd.read_excel(filepath + '\\SBSU.xlsx')
#SBSU_ticker_list = list(SBSU_data['BBG_STOCK_TICKER']) 


#    # Parse the html details collected from GET request #
#    soup = BeautifulSoup(r.text, 'html.parser')
#    # Find the first table that matches the specific class parameters #
#    table = soup.find_all('table', {'class': 'tabBodyLV17'})[0]
#    # Get all row details from the first table #
#    rows = table.find_all('tr')[1:]
#    for row in rows:
#        data = row.find('td').find_next_siblings()
#        if country in data[0].img['title']:
#            url = 'https://www.marketscreener.com' + str(data[1].a['href']) + 'company/'
#            r = requests.get(url)
    
### Initialize variables ##
#reuters_url = 'https://www.reuters.com/finance/stocks/lookup?searchType=any&comSortBy=marketcap&sortBy=&dateRange=&search='
#filepath = 'C:\\Users\\C_YEE\\Projects\\data\\News' 
#
### Load Scientific Beta Stock Universe ##
#SBSU_data = pd.read_excel(filepath + '\\SBSU.xlsx')
#SBSU_ticker_list = list(SBSU_data['BBG_STOCK_TICKER'])
#SBSU_data['REUTERS_COMPANY'] = np.nan
#SBSU_data['REUTERS_RIC'] = np.nan
#SBSU_data['REUTERS_EXCHANGE'] = np.nan
#SBSU_data['SIMILARITY_RATIO'] = np.nan
#counter = 0
#
## Parse the first table with each ticker #
#for ticker in SBSU_ticker_list:
#    table = pd.read_html(reuters_url + str(ticker), header = 0)[0]
#    # Update the respective columns in SBSU_data #
#    SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER'] == ticker, ['REUTERS_COMPANY']] = table.iloc[0,0]
#    SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER'] == ticker, ['REUTERS_RIC']] = table.iloc[0,1].split('.')[0]
#    SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER'] == ticker, ['REUTERS_EXCHANGE']] = table.iloc[0,2]
#    SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER'] == ticker, ['SIMILARITY_RATIO']] = \
#    lev.ratio(SBSU_data.loc[SBSU_data['BBG_STOCK_TICKER'] == ticker, ['companyName']]['companyName'].iloc[0] , table.iloc[0,0])
#    counter += 1
#    print('Progress: %d of %d' % (counter, len(SBSU_data)))
